{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Cell to run prior to running following cells in notebook during development.\n",
    "# Delete this cell prior to publishing.\n",
    "stereodir=/global/scratch/users/rsprouse/yidcorp/audio/stereo\n",
    "tgtdir=/global/home/groups/fc_phonlab/spkrcorpus\n",
    "dur=45\n",
    "\n",
    "# Clear out existing files.\n",
    "rm -rf ${tgtdir}/audio\n",
    "rm -rf ${tgtdir}/diarized\n",
    "\n",
    "# Speaker 1\n",
    "spkrdir=${tgtdir}/audio/stereo/speaker_1\n",
    "mkdir -p ${spkrdir}\n",
    "sox ${stereodir}/Moishe_Gorelik_Tape4.wav ${spkrdir}/interview_a.wav trim 0 ${dur}\n",
    "sox ${stereodir}/Moishe_Gorelik_Tape5.wav ${spkrdir}/interview_b.wav trim 0 ${dur}\n",
    "\n",
    "# Speaker 2\n",
    "spkrdir=${tgtdir}/audio/stereo/speaker_2\n",
    "mkdir -p ${spkrdir}\n",
    "sox ${stereodir}/Zhenya_Raykhman_Tape2.wav ${spkrdir}/interview_a.wav trim 0 ${dur}\n",
    "sox ${stereodir}/Zhenya_Raykhman_Tape3.wav ${spkrdir}/interview_b.wav trim 0 ${dur}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import diarize_utils as utils\n",
    "from pyannote.audio import Pipeline\n",
    "from phonlab.utils import dir2df\n",
    "from audiolabel import df2tg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare audio files and diarize them\n",
    "\n",
    "This notebook outlines a sample workflow that can be used to prepare a set of audio files for diarization and then run a diarization pipeline on them to produce annotation files (`.eaf` for use with [ELAN](https://archive.mpi.nl/tla/elan); `.TextGrid` for use with [Praat](https://www.fon.hum.uva.nl/praat/)). The workflow produces speaker tiers in the annotation files that contain intervals that mark locations in the audio where the speaker is talking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: image of sample textgrid/eaf output + waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The overall workflow\n",
    "\n",
    "The workflow consists of two steps:\n",
    "\n",
    "1. [Preparation of the audio files for input to the diarization process](#step-1).\n",
    "1. [Diarization of the pre-processed audio files](#step-2).\n",
    "\n",
    "Each step is designed to be easily repeated whenever new inputs to the step are added to the project. For the first step this would be when new source audio files are added. For the second step this would be when the first step has pre-processed new files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project organization\n",
    "\n",
    "The input and output files in the project corpus are stored as subdirectories of the project root, which is defined by `projroot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "projroot = Path('/global/home/groups/fc_phonlab/spkrcorpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input audio files\n",
    "\n",
    "The source audio files are stereo files in the `audio/stereo` directory that are organized by speaker. The [`dir2df` function](https://github.com/rsprouse/phonlab/blob/master/doc/Retrieving%20filenames%20in%20a%20directory%20tree%20with%20%60dir2df()%60.ipynb) produces a dataframe of the files found in the source audio directory. The speaker subdirectories are shown by the value of the `relpath` column, and the filenames are stored in `fname`. The `barename` column contains the filename without its extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relpath</th>\n",
       "      <th>fname</th>\n",
       "      <th>barename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>speaker_1</td>\n",
       "      <td>interview_a.wav</td>\n",
       "      <td>interview_a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>speaker_1</td>\n",
       "      <td>interview_b.wav</td>\n",
       "      <td>interview_b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>speaker_2</td>\n",
       "      <td>interview_a.wav</td>\n",
       "      <td>interview_a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>speaker_2</td>\n",
       "      <td>interview_b.wav</td>\n",
       "      <td>interview_b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     relpath            fname     barename\n",
       "0  speaker_1  interview_a.wav  interview_a\n",
       "1  speaker_1  interview_b.wav  interview_b\n",
       "2  speaker_2  interview_a.wav  interview_a\n",
       "3  speaker_2  interview_b.wav  interview_b"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir2df(\n",
    "    projroot/'audio'/'stereo',\n",
    "    addcols=['barename']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processed audio files\n",
    "\n",
    "The first step in the workflow extracts a channel from the stereo `.wav` files and downsamples it. Since the input files are stereo files the left and right channels are extracted separately before downsampling.\n",
    "\n",
    "`channelmap` defines a mapping of input audio channel to output directory. Here channel `1` will map to the `audio/left` subdirectory, and channel `2` maps to `audio/right`.\n",
    "\n",
    "For mono input files a simpler `channelmap` can be created that maps channel `1` to a subdirectory name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "channelmap = {\n",
    "    1: 'left',\n",
    "    2: 'right'\n",
    "}\n",
    "\n",
    "#channelmap = {1: 'downsampled'} # A sample `channelmap` for mono input audio files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diarized outputs\n",
    "\n",
    "In the second step of the workflow the pre-processed files are diarized. The `output_type` selects `TextGrid` or `eaf` output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_type = 'TextGrid' # Desired output type: 'eaf' or 'TextGrid'\n",
    "num_speakers = 2  # Per channel\n",
    "buffer = 0.250 # In seconds\n",
    "# Text to include in labelled speech regions.\n",
    "speech_label = '*' if output_type == 'TextGrid' else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: more on auth tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenfile = projroot/'pyannote-auth-token'\n",
    "with open(tokenfile, 'r') as tf:\n",
    "    auth_token = tf.readline().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"step-1\">Step 1: Extract the channels and downsample audio</a>\n",
    "\n",
    "The output audio files will consist of a single channel from an input audio file that has been downsampled to 16000 Hz, which matches the sample rate used to train the diarization model. (TODO: verify)\n",
    "\n",
    "The `compare_dirs` function finds `stereo` files that do not yet have corresponding `left` or `right` output files. The `ext1` and `ext2` values ensure that `compare_dirs` only looks for `.wav` files in the corresponding directories. `compare_dirs` returns a dataframe in which each row contains a file that requires processing.\n",
    "\n",
    "We iterate over the rows of the `todo` dataframe and use `prep_audio` to extract one channel of audio and downsample. The resulting `.wav` file is stored in a `left` or `right` subdirectory. The inclusion of `relpath` in the output filepath also ensures that the `speaker` directory structure is replicated in the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prep_audio: /global/home/groups/fc_phonlab/spkrcorpus/audio/left/speaker_1/interview_a.wav\n",
      "prep_audio: /global/home/groups/fc_phonlab/spkrcorpus/audio/left/speaker_1/interview_b.wav\n",
      "prep_audio: /global/home/groups/fc_phonlab/spkrcorpus/audio/left/speaker_2/interview_a.wav\n",
      "prep_audio: /global/home/groups/fc_phonlab/spkrcorpus/audio/left/speaker_2/interview_b.wav\n",
      "prep_audio: /global/home/groups/fc_phonlab/spkrcorpus/audio/right/speaker_1/interview_a.wav\n",
      "prep_audio: /global/home/groups/fc_phonlab/spkrcorpus/audio/right/speaker_1/interview_b.wav\n",
      "prep_audio: /global/home/groups/fc_phonlab/spkrcorpus/audio/right/speaker_2/interview_a.wav\n",
      "prep_audio: /global/home/groups/fc_phonlab/spkrcorpus/audio/right/speaker_2/interview_b.wav\n"
     ]
    }
   ],
   "source": [
    "verbose = True   # Set to false to suppress progress messages\n",
    "\n",
    "# Loop over the channels defined in `channelmap`.\n",
    "for chan_num, chan_name in channelmap.items():\n",
    "    srcdir = projroot/'audio'/'stereo'\n",
    "    chandir = projroot/'audio'/chan_name\n",
    "\n",
    "    # Find input stereo files that don't have a corresponding\n",
    "    # left|right pre-processed file.\n",
    "    todo = utils.compare_dirs(\n",
    "        dir1=srcdir, ext1='.wav',\n",
    "        dir2=chandir, ext2='.wav'\n",
    "    )\n",
    "\n",
    "    # Loop over the files that require processing.\n",
    "    for row in todo.itertuples():\n",
    "        infile = srcdir/row.relpath/row.fname\n",
    "        outfile = chandir/row.relpath/row.fname\n",
    "\n",
    "        # Create pre-processed output file for left|right channel.\n",
    "        if verbose:\n",
    "            print(f'prep_audio: {outfile}')\n",
    "        utils.prep_audio(infile, outfile, chan_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"step-2\">Step 2: Diarize the pre-processed files</a>\n",
    "\n",
    "### Instantiate the pipeline\n",
    "\n",
    "TODO: more on setting params.\n",
    "\n",
    "Note that `pipeline` only needs to be instantiated once. The cell that performs diarization can be executed repeatedly without re-instantiating `pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyannote.audio.pipelines.speaker_diarization.SpeakerDiarization at 0x2b83aabc5f70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization\",\n",
    "    use_auth_token=auth_token\n",
    ")\n",
    "parameters = {\n",
    "    \"segmentation\": {\n",
    "        \"min_duration_off\": 0.3,\n",
    "    },\n",
    "}\n",
    "\n",
    "pipeline.instantiate(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diarize the pre-processed audio\n",
    "\n",
    "The outputs of diarization are annotation files for each of the pre-processed audio files.\n",
    "\n",
    "First `compare_dirs` function finds pre-processed left|right `.wav` files that do not have a corresponding `TextGrid`. The `ext1` and `ext2` values ensure that `compare_dirs` only looks for `.wav` and `TextGrid` files in their corresponding directories.\n",
    "\n",
    "Each `TextGrid` is created by the `diarize` function while iterating over `todo`. Note that the output filename is constructed by `barename` (the input file's filename without extension) and using `TextGrid` as the extension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diarize: /global/home/groups/fc_phonlab/spkrcorpus/diarized/TextGrid/left/speaker_1/interview_a.TextGrid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diarize: /global/home/groups/fc_phonlab/spkrcorpus/diarized/TextGrid/left/speaker_1/interview_b.TextGrid\n",
      "diarize: /global/home/groups/fc_phonlab/spkrcorpus/diarized/TextGrid/left/speaker_2/interview_a.TextGrid\n",
      "diarize: /global/home/groups/fc_phonlab/spkrcorpus/diarized/TextGrid/left/speaker_2/interview_b.TextGrid\n",
      "diarize: /global/home/groups/fc_phonlab/spkrcorpus/diarized/TextGrid/right/speaker_1/interview_a.TextGrid\n",
      "diarize: /global/home/groups/fc_phonlab/spkrcorpus/diarized/TextGrid/right/speaker_1/interview_b.TextGrid\n",
      "diarize: /global/home/groups/fc_phonlab/spkrcorpus/diarized/TextGrid/right/speaker_2/interview_a.TextGrid\n",
      "diarize: /global/home/groups/fc_phonlab/spkrcorpus/diarized/TextGrid/right/speaker_2/interview_b.TextGrid\n"
     ]
    }
   ],
   "source": [
    "# Loop over the channels defined in `channelmap`.\n",
    "for chan_num, chan_name in channelmap.items():\n",
    "    wavdir = projroot/'audio'/chan_name\n",
    "    outdir = projroot/'diarized'/output_type/chan_name\n",
    "\n",
    "    # Find input pre-processed files that don't have a corresponding\n",
    "    # left|right TextGrid.\n",
    "    todo = utils.compare_dirs(\n",
    "        dir1=wavdir, ext1='.wav',\n",
    "        dir2=outdir, ext2=f'.{output_type}'\n",
    "    )\n",
    "\n",
    "    # Loop over the files that require processing.\n",
    "    for row in todo.itertuples():\n",
    "        wavfile = wavdir/row.relpath/row.fname\n",
    "        outfile = outdir/row.relpath/f'{row.barename}.{output_type}'\n",
    "\n",
    "        # Create TextGrid for left|right pre-processed audio file.\n",
    "        if verbose:\n",
    "            print(f'diarize: {outfile}')\n",
    "        diarization = utils.diarize(\n",
    "            wavfile, pipeline, outfile, num_speakers, buffer, speech_label\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"step-3\">Step 3: Combine diarized outputs</a>\n",
    "\n",
    "In this step we combine the tiers in the `left` and `right` output files into a single `TextGrid` or `eaf` file.\n",
    "\n",
    "We also use a sorting algorithm to assign labels to the combined tiers. For the interviews in our corpus we expect the person doing the most speaking to be the person being interviewed. We also expect the average intensity of each person's speech to be greater in the channel corresponding to the closer microphone. On the basis of this sorting we assign `Subject probable`, `Subject unlikely`, `Interviewer probable`, and `Interviewer unlikely` to the annotation tiers.\n",
    "\n",
    "If the sorting algorithm fails to cleanly find the subject and interviewer on separate channels, then `unknown` names are assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted: /global/home/groups/fc_phonlab/spkrcorpus/diarized/TextGrid/combined/speaker_1/interview_a.TextGrid\n",
      "sorted: /global/home/groups/fc_phonlab/spkrcorpus/diarized/TextGrid/combined/speaker_1/interview_b.TextGrid\n",
      "sorted: /global/home/groups/fc_phonlab/spkrcorpus/diarized/TextGrid/combined/speaker_2/interview_a.TextGrid\n",
      "sorted: /global/home/groups/fc_phonlab/spkrcorpus/diarized/TextGrid/combined/speaker_2/interview_b.TextGrid\n"
     ]
    }
   ],
   "source": [
    "# Find existing left/right label files that need to be combined.\n",
    "annodir = projroot/'diarized'/output_type\n",
    "(annodir/'combined').mkdir(parents=True, exist_ok=True)\n",
    "todo = utils.compare_dirs(\n",
    "    dir1=annodir/'left', ext1=output_type,\n",
    "    dir2=annodir/'combined', ext2=output_type\n",
    ")\n",
    "\n",
    "# Loop over label files to be combined and sort the tiers.\n",
    "for row in todo.itertuples():\n",
    "    tierdfs, tiernames = utils.sort_tiers(\n",
    "        annodir,\n",
    "        list(channelmap.values()),\n",
    "        projroot/'audio'/'stereo',\n",
    "        row.relpath,\n",
    "        row.fname\n",
    "    )\n",
    "    outfile = annodir/'combined'/row.relpath/f'{row.barename}.{output_type}'\n",
    "    outfile.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if verbose:\n",
    "        print(f'sorted: {outfile}')\n",
    "    if output_type == 'TextGrid':\n",
    "        df2tg(\n",
    "            tierdfs,\n",
    "            tnames=tiernames,\n",
    "            lbl='label',\n",
    "            ftype='praat_short',\n",
    "            outfile=outfile\n",
    "        )\n",
    "    elif output_type == 'eaf':\n",
    "        utils.write_eaf(tierdfs, tiernames, outfile, speech_label, 't1', 't2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diarize",
   "language": "python",
   "name": "diarize"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 261.85,
   "position": {
    "height": "40px",
    "left": "859px",
    "right": "20px",
    "top": "54px",
    "width": "313px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
